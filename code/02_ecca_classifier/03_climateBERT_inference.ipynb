{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on colab: False\n"
     ]
    }
   ],
   "source": [
    "# check if on colab\n",
    "try:\n",
    "    import google.colab\n",
    "    COLAB = True\n",
    "except:\n",
    "    COLAB=False\n",
    "print('on colab:', COLAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/malo/Library/CloudStorage/GoogleDrive-malo.jan@sciencespo.fr/.shortcut-targets-by-id/17Ie9FRNLITIRVq7pV8zYiq6P5Op2F6Lu/edf_un_climate\n"
     ]
    }
   ],
   "source": [
    "# set working directory to base of repo\n",
    "\n",
    "import os \n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  # Might need to adapt to your path\n",
    "    if not os.getcwd().endswith(\"edf_un_climate\"):\n",
    "        os.chdir(\"drive/MyDrive/edf_un_climate\") \n",
    "else:\n",
    "    if not os.getcwd().endswith(\"edf_un_climate\"):\n",
    "        os.chdir(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "    print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/malo/anaconda3/envs/un_climate/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from tqdm.auto import tqdm\n",
    "from torch.nn.functional import softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/un_speeches_tokenized.parquet\", engine = \"pyarrow\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that classify sentences using a Hugging Face model\n",
    "\n",
    "def classify_climate(df, text_column, model_name, batch_size=32):\n",
    "    \"\"\"\n",
    "    Run inference on a pandas DataFrame using a Hugging Face model with batch processing.\n",
    "\n",
    "    :param df: pandas DataFrame containing the text data.\n",
    "    :param text_column: Name of the column in df that contains the text.\n",
    "    :param model_name: Name of the model on Hugging Face Model Hub.\n",
    "    :param batch_size: Size of batches for processing.\n",
    "    :return: DataFrame with original data and new columns 'prediction' and 'probability'.\n",
    "    \"\"\"\n",
    "    # Load model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "\n",
    "    # Prepare device (GPU or CPU)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(0, len(df), batch_size), desc=\"Processing\"):\n",
    "        batch_texts = df.iloc[i:i + batch_size][text_column].tolist()\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            probs = softmax(outputs.logits, dim=1)\n",
    "            predictions = torch.argmax(probs, dim=1).tolist()\n",
    "            probabilities = probs.max(dim=1).values.tolist()\n",
    "\n",
    "        results.extend(zip(df.iloc[i:i + batch_size]['id'], predictions, probabilities))\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results, columns=['id', 'prediction_climateBert', 'probability_climateBert'])\n",
    "\n",
    "    # Merge with original DataFrame\n",
    "    final_df = df.merge(results_df, on='id', how='left')\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the climateBERT from Huggingface and use it to classify the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 40267/40267 [3:38:18<00:00,  3.07it/s]  \n"
     ]
    }
   ],
   "source": [
    "model_name = \"climatebert/distilroberta-base-climate-detector\" \n",
    "df_predicted = classify_climate(df, 'text', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['As a member of the Security Council, Australia was also a member of the Atomic Energy Commission, established by the General Assembly at the first part of the first session.',\n",
       " 'It is plain that each and every nation entering into the atomic energy agreement must be bound by all its obligations.',\n",
       " 'The air we breathe is more invigorating.',\n",
       " 'It is therefore to be expected that during the coming weeks we must work with unremitting energy in order to complete our work.',\n",
       " 'I wish to say a word now about the work of the Atomic Energy Commission.',\n",
       " 'There is a great diversity of opinions regarding the results of the Paris Conference.',\n",
       " 'We must ensure, by all means, that the smouldering Spanish problem does not become the bone of contention between East and West.',\n",
       " 'In conclusion, I would like to mention the Conference of Paris.',\n",
       " 'Now we may look back and measure the distance already covered.',\n",
       " 'During the last few months the United Nations has had to examine one of the most tormenting problems of our times, that of atomic energy.']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted[df_predicted['prediction_climateBert'] == 1].text.tolist()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prediction_climateBert\n",
       "0    1221922\n",
       "1      66591\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_predicted.value_counts('prediction_climateBert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save predictions of climateBert\n",
    "\n",
    "df_predicted.to_parquet(\"data/un_predictions_climateBert.parquet\", engine = \"pyarrow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "un_climate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
